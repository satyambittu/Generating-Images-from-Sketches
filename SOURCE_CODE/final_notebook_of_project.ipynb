{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:26.926140Z",
     "iopub.status.busy": "2024-11-16T13:11:26.925713Z",
     "iopub.status.idle": "2024-11-16T13:11:27.316203Z",
     "shell.execute_reply": "2024-11-16T13:11:27.315449Z",
     "shell.execute_reply.started": "2024-11-16T13:11:26.926092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:27.318517Z",
     "iopub.status.busy": "2024-11-16T13:11:27.318029Z",
     "iopub.status.idle": "2024-11-16T13:11:31.975579Z",
     "shell.execute_reply": "2024-11-16T13:11:31.974622Z",
     "shell.execute_reply.started": "2024-11-16T13:11:27.318473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:31.977274Z",
     "iopub.status.busy": "2024-11-16T13:11:31.976843Z",
     "iopub.status.idle": "2024-11-16T13:11:32.011195Z",
     "shell.execute_reply": "2024-11-16T13:11:32.010211Z",
     "shell.execute_reply.started": "2024-11-16T13:11:31.977239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device=('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:32.014207Z",
     "iopub.status.busy": "2024-11-16T13:11:32.013813Z",
     "iopub.status.idle": "2024-11-16T13:11:32.022890Z",
     "shell.execute_reply": "2024-11-16T13:11:32.022101Z",
     "shell.execute_reply.started": "2024-11-16T13:11:32.014162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SketchToFaceDataset(Dataset):\n",
    "    def __init__(self, sketch_images, attributes, real_images, transform=None):\n",
    "        self.sketch_images = sketch_images  \n",
    "        self.attributes = attributes  \n",
    "        self.real_images = real_images  \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sketch_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sketch = self.sketch_images[idx]\n",
    "        attr = self.attributes[idx]\n",
    "        real_img = self.real_images[idx]\n",
    "        \n",
    "#         if self.transform:\n",
    "#             sketch = self.transform(sketch)\n",
    "#             real_img = self.transform(real_img)\n",
    "        return sketch, attr, real_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:32.024188Z",
     "iopub.status.busy": "2024-11-16T13:11:32.023897Z",
     "iopub.status.idle": "2024-11-16T13:11:32.134303Z",
     "shell.execute_reply": "2024-11-16T13:11:32.133499Z",
     "shell.execute_reply.started": "2024-11-16T13:11:32.024158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/image-dict-3/images_dict_3.pkl', 'rb') as f:\n",
    "    dict_image = pickle.load(f)\n",
    "    \n",
    "\n",
    "df_atrb=pd.read_csv('/kaggle/input/atributes-list-40/df_atributes_40_columns.csv')\n",
    "\n",
    "root_dir='/kaggle/input/celeba/img_align_celeba'\n",
    "real_images=[]\n",
    "for idx in range(len(df_atrb)):\n",
    "    image_name=df_atrb.iloc[idx,1]\n",
    "    image_path=os.path.join(root_dir,image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "    resized_image = cv2.resize(image, (128, 128))\n",
    "    reshaped_image = np.transpose(resized_image, (2, 0, 1))\n",
    "    real_images.append(reshaped_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:32.135643Z",
     "iopub.status.busy": "2024-11-16T13:11:32.135350Z",
     "iopub.status.idle": "2024-11-16T13:11:32.522901Z",
     "shell.execute_reply": "2024-11-16T13:11:32.521872Z",
     "shell.execute_reply.started": "2024-11-16T13:11:32.135611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sketch_images=dict_image.values()\n",
    "sketch_images=list(sketch_images)\n",
    "attributes=np.array(df_atrb.iloc[:,2:20])\n",
    "\n",
    "sketch_images = torch.tensor(sketch_images, dtype=torch.float32).to(device).unsqueeze(1)  \n",
    "attributes = torch.tensor(attributes, dtype=torch.float32).to(device)  \n",
    "real_images = torch.tensor(real_images, dtype=torch.float32).to(device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:32.524445Z",
     "iopub.status.busy": "2024-11-16T13:11:32.524129Z",
     "iopub.status.idle": "2024-11-16T13:11:32.529914Z",
     "shell.execute_reply": "2024-11-16T13:11:32.528695Z",
     "shell.execute_reply.started": "2024-11-16T13:11:32.524412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Create dataset\n",
    "dataset = SketchToFaceDataset(sketch_images, attributes, real_images, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:32.531418Z",
     "iopub.status.busy": "2024-11-16T13:11:32.531060Z",
     "iopub.status.idle": "2024-11-16T13:11:32.552283Z",
     "shell.execute_reply": "2024-11-16T13:11:32.551339Z",
     "shell.execute_reply.started": "2024-11-16T13:11:32.531387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        #  Sketch processing\n",
    "        self.conv1_A = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)  \n",
    "        self.conv2_A = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1) \n",
    "        self.conv3_A = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1) \n",
    "        self.prelu_A = nn.PReLU()\n",
    "\n",
    "        # Attribute processing\n",
    "        self.fc = nn.Linear(18, 128 * 128)  \n",
    "        self.conv1_B = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)  \n",
    "        self.conv2_B = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)  \n",
    "        self.conv3_B = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)  \n",
    "        self.prelu_B = nn.PReLU()\n",
    "\n",
    "        # Downsampling\n",
    "        self.conv1_down = nn.Conv2d(384, 64, kernel_size=3, stride=2, padding=1) \n",
    "        self.conv2_down = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1) \n",
    "        self.conv3_down = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1) \n",
    "        self.conv4_down = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1) \n",
    "        self.prelu_down = nn.PReLU()\n",
    "\n",
    "        # Upsampling\n",
    "        self.deconv1_up = nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1)  \n",
    "        self.deconv2_up = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1) \n",
    "        self.deconv3_up = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1) \n",
    "        self.deconv4_up = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)  \n",
    "        self.conv_out = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)  \n",
    "        self.prelu_up = nn.PReLU()\n",
    "\n",
    "    def forward(self, sketch_image, attributes):\n",
    "        \n",
    "        out1_A = self.prelu_A(self.conv1_A(sketch_image)) \n",
    "        out2_A = self.prelu_A(self.conv2_A(out1_A)) \n",
    "        out3_A = self.prelu_A(self.conv3_A(out2_A))  \n",
    "        \n",
    "       \n",
    "        out_A = torch.cat((out1_A, out2_A, out3_A), dim=1) \n",
    "        \n",
    "        \n",
    "        attr = self.fc(attributes)\n",
    "        attr = attr.view(attributes.size(0), 1, 128, 128)  \n",
    "        out1_B = self.prelu_B(self.conv1_B(attr)) \n",
    "        out2_B = self.prelu_B(self.conv2_B(out1_B))\n",
    "        out3_B = self.prelu_B(self.conv3_B(out2_B)) \n",
    "        \n",
    "        \n",
    "        out_B = torch.cat((out1_B, out2_B, out3_B), dim=1) \n",
    "\n",
    "        \n",
    "        combined = torch.cat((out_A, out_B), dim=1) \n",
    "\n",
    "        \n",
    "        down1 = self.prelu_down(self.conv1_down(combined)) \n",
    "        down2 = self.prelu_down(self.conv2_down(down1)) \n",
    "        down3 = self.prelu_down(self.conv3_down(down2))  \n",
    "        down4 = self.prelu_down(self.conv4_down(down3))  \n",
    "\n",
    "        \n",
    "        up1 = self.prelu_up(self.deconv1_up(down4))  \n",
    "        up1 = torch.cat((up1, down3), dim=1) \n",
    "\n",
    "        up2 = self.prelu_up(self.deconv2_up(up1))  \n",
    "        up2 = torch.cat((up2, down2), dim=1)  \n",
    "\n",
    "        up3 = self.prelu_up(self.deconv3_up(up2)) \n",
    "        up3 = torch.cat((up3, down1), dim=1) \n",
    "\n",
    "        up4 = self.prelu_up(self.deconv4_up(up3)) \n",
    "        \n",
    "       \n",
    "        out = self.conv_out(up4)  \n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:32.553525Z",
     "iopub.status.busy": "2024-11-16T13:11:32.553229Z",
     "iopub.status.idle": "2024-11-16T13:11:32.566675Z",
     "shell.execute_reply": "2024-11-16T13:11:32.565780Z",
     "shell.execute_reply.started": "2024-11-16T13:11:32.553495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Convolutional layers to process the face image\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "        )\n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(18, 256)\n",
    "\n",
    "       \n",
    "        self.out = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0)\n",
    "        \n",
    "        self.linear=nn.Linear(169,13)\n",
    "        self.linear2=nn.Linear(13,1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self, face_image, attributes):\n",
    "       \n",
    "        out_img = self.conv(face_image)\n",
    "\n",
    "        \n",
    "        attr = self.fc(attributes)\n",
    "        attr = attr.unsqueeze(2).unsqueeze(3) \n",
    "        attr = attr.expand(attr.size(0), attr.size(1), out_img.size(2), out_img.size(3))\n",
    "\n",
    "        \n",
    "        combined = torch.cat([out_img, attr], dim=1)\n",
    "\n",
    "        \n",
    "        validity = self.out(combined)\n",
    "        validity=validity.view(-1,169)\n",
    "        l1=self.linear(validity)\n",
    "        final=self.sigmoid(self.linear2(l1))\n",
    "        return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:32.570703Z",
     "iopub.status.busy": "2024-11-16T13:11:32.570430Z",
     "iopub.status.idle": "2024-11-16T13:11:32.579946Z",
     "shell.execute_reply": "2024-11-16T13:11:32.579179Z",
     "shell.execute_reply.started": "2024-11-16T13:11:32.570673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Reconstruction loss (pixel-wise loss)\n",
    "# def reconstruction_loss(gen_images, real_images):\n",
    "#     return F.mse_loss(gen_images, real_images)\n",
    "\n",
    "# Adversarial loss (for real/fake classification)\n",
    "adversarial_loss = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:32.581530Z",
     "iopub.status.busy": "2024-11-16T13:11:32.581152Z",
     "iopub.status.idle": "2024-11-16T13:11:32.594611Z",
     "shell.execute_reply": "2024-11-16T13:11:32.593757Z",
     "shell.execute_reply.started": "2024-11-16T13:11:32.581490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"/kaggle/working/\"\n",
    "checkpoint_interval=1\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "def train(generator, discriminator, dataloader, num_epochs=50, lr=0.0002):\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (sketches, attributes, real_faces) in enumerate(dataloader):\n",
    "            batch_size = sketches.size(0)\n",
    "            sketches=sketches.to(device)\n",
    "            attributes=attributes.to(device)\n",
    "            real_faces=real_faces.to(device)\n",
    "            \n",
    "            valid = torch.ones((batch_size, 1), requires_grad=False).to(device)\n",
    "            fake = torch.zeros((batch_size, 1), requires_grad=False).to(device)\n",
    "\n",
    "           \n",
    "            #  Train Generator\n",
    "     \n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            \n",
    "            gen_faces = generator(sketches, attributes)\n",
    "\n",
    "            \n",
    "            g_loss_adv = adversarial_loss(discriminator(gen_faces, attributes), valid)\n",
    "            \n",
    "#             g_loss_recon = reconstruction_loss(gen_faces, real_faces)\n",
    "\n",
    "           \n",
    "            g_loss = g_loss_adv #+ g_loss_recon\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        \n",
    "            \n",
    "         \n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            \n",
    "            real_loss = adversarial_loss(discriminator(real_faces, attributes), valid)\n",
    "            \n",
    "            fake_loss = adversarial_loss(discriminator(gen_faces.detach(), attributes), fake)\n",
    "            \n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
    "\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            torch.save(generator.state_dict(), f\"{CHECKPOINT_DIR}/generator_epoch_{epoch+1}.pth\")\n",
    "            torch.save(discriminator.state_dict(), f\"{CHECKPOINT_DIR}/discriminator_epoch_{epoch+1}.pth\")\n",
    "            print(f\"Checkpoint saved at epoch {epoch+1}.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:32.596375Z",
     "iopub.status.busy": "2024-11-16T13:11:32.595714Z",
     "iopub.status.idle": "2024-11-16T13:11:32.607599Z",
     "shell.execute_reply": "2024-11-16T13:11:32.606747Z",
     "shell.execute_reply.started": "2024-11-16T13:11:32.596334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def inference(generator_checkpoint, sketch_image, attributes, device):\n",
    "    generator = Generator().to(device)\n",
    "    generator.load_state_dict(torch.load(generator_checkpoint, map_location=device))\n",
    "    generator.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sketch_image = sketch_image.to(device)\n",
    "        attributes = attributes.to(device)\n",
    "        generated_image = generator(sketch_image, attributes)\n",
    "    \n",
    "    return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:32.608990Z",
     "iopub.status.busy": "2024-11-16T13:11:32.608690Z",
     "iopub.status.idle": "2024-11-16T13:11:32.659133Z",
     "shell.execute_reply": "2024-11-16T13:11:32.658462Z",
     "shell.execute_reply.started": "2024-11-16T13:11:32.608959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:32.660580Z",
     "iopub.status.busy": "2024-11-16T13:11:32.660211Z",
     "iopub.status.idle": "2024-11-16T13:11:33.867849Z",
     "shell.execute_reply": "2024-11-16T13:11:33.866934Z",
     "shell.execute_reply.started": "2024-11-16T13:11:32.660540Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/2] [Batch 0/2] [D loss: 0.6769] [G loss: 0.6577]\n",
      "[Epoch 0/2] [Batch 1/2] [D loss: 0.3946] [G loss: 0.6345]\n",
      "Checkpoint saved at epoch 1.\n",
      "[Epoch 1/2] [Batch 0/2] [D loss: 0.3531] [G loss: 0.7073]\n",
      "[Epoch 1/2] [Batch 1/2] [D loss: 0.3094] [G loss: 0.7761]\n",
      "Checkpoint saved at epoch 2.\n"
     ]
    }
   ],
   "source": [
    "train(generator, discriminator, dataloader, num_epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T13:11:33.869778Z",
     "iopub.status.busy": "2024-11-16T13:11:33.869499Z",
     "iopub.status.idle": "2024-11-16T13:11:33.969259Z",
     "shell.execute_reply": "2024-11-16T13:11:33.968338Z",
     "shell.execute_reply.started": "2024-11-16T13:11:33.869747Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated image saved as 'generated_image.png'.\n"
     ]
    }
   ],
   "source": [
    "example_sketch = torch.randn(1, 1, 128, 128)  \n",
    "example_attributes = torch.randn(1, 18)       \n",
    "generated_image = inference(f\"{CHECKPOINT_DIR}/generator_epoch_2.pth\", example_sketch, example_attributes, device)\n",
    "\n",
    "# Save the generated image\n",
    "from torchvision.utils import save_image\n",
    "save_image(generated_image, \"generated_image.png\")\n",
    "print(\"Generated image saved as 'generated_image.png'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1176847,
     "sourceId": 1970400,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5910571,
     "sourceId": 9671947,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5911251,
     "sourceId": 9672840,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5914136,
     "sourceId": 9676612,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6057145,
     "sourceId": 9867953,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6057240,
     "sourceId": 9868075,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
